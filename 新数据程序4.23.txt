

train_data = pd.read_csv(r"C:\Users\admin\Desktop\new_data\train_spectrum.csv", header=None) 
train_data = train_data.fillna(method='pad',axis=1) 
train_data=train_data.values  
train_data=train_data[:,1:6144]

train_lable = pd.read_csv(r"C:\Users\admin\Desktop\new_data\train_content.csv", header=None) 
train_lable = train_lable.fillna(method='pad',axis=1) 
train_lable=train_lable.values 
train_lable=train_lable[:,0]

row,column=np.where(np.isnan(train_data))
train_lable= np.delete(train_lable, row, axis=0) 
train_data= np.delete(train_data, row, axis=0) 
del_lable_0=np.where(train_lable==0)
train_lable= np.delete(train_lable, del_lable_0, axis=0) 
train_data= np.delete(train_data, del_lable_0, axis=0)
row=np.where(np.isnan(train_lable))
train_lable= np.delete(train_lable, row, axis=0) 
train_data= np.delete(train_data, row, axis=0) 
train_data=preprocessing.minmax_scale(train_data,feature_range=(0,1),axis=1)

test_data = pd.read_csv(r"C:\Users\admin\Desktop\new_data\test_spectrum.csv", header=None) 
test_data = test_data.fillna(method='pad',axis=1) 
test_data=test_data.values  
test_data=test_data[:,1:6144]

test_lable = pd.read_csv(r"C:\Users\admin\Desktop\new_data\test_content.csv", header=None) 
test_lable = test_lable.fillna(method='pad',axis=1) 
test_lable=test_lable.values[:,0] 

row,column=np.where(np.isnan(test_data))
test_lable= np.delete(test_lable, row, axis=0) 
test_data= np.delete(test_data, row, axis=0)
del_lable_0=np.where(test_lable==0)
test_lable= np.delete(test_lable, del_lable_0, axis=0) 
test_data= np.delete(test_data, del_lable_0, axis=0)
row=np.where(np.isnan(test_lable))
test_lable= np.delete(test_lable, row, axis=0) 
test_data= np.delete(test_data, row, axis=0) 
test_data=preprocessing.minmax_scale(test_data,feature_range=(0,1),axis=1)

best_gamma=1.6666666666666665
best_kernel="poly"

kpca = KernelPCA(n_components=1000,fit_inverse_transform=True,kernel=best_kernel,gamma=best_gamma)
kpca.fit(train_data)
train_data=kpca.transform(train_data)
test_data=kpca.transform(test_data)


train_data=train_data.reshape(train_data.shape[0],1,1000,1) 
test_data=test_data.reshape(test_data.shape[0],1,1000,1)
train_db = tf.data.Dataset.from_tensor_slices((train_data, train_lable))
train_db = train_db.shuffle(10000)
train_db = train_db.batch(100)  #加入b维度
test_db = tf.data.Dataset.from_tensor_slices((test_data, test_lable))
test_db = test_db.batch(100)  #加入b维度

network = Sequential([ # 封装为一个网络 layers.LeakyReLU(alpha=0.1),
 layers.Conv2D(20,kernel_size=(1,3),strides=(1,1),dilation_rate=2,kernel_initializer='he_normal',bias_initializer='zeros',padding='same',data_format='channels_first'),
 #layers.BatchNormalization(),
    layers.MaxPooling2D(pool_size=(1,2), strides=(1, 1), padding='same',data_format='channels_first'),
 layers.Conv2D(8,kernel_size=(1,5), strides=(1,1),dilation_rate=3,kernel_initializer='he_normal',bias_initializer='zeros',padding='same',data_format='channels_first'),
 #layers.BatchNormalization(),
    layers.MaxPooling2D(pool_size=(1,4), strides=(1, 1), padding='same',data_format='channels_first'),
 layers.Conv2D(8,kernel_size=(1,5),strides=(1,1),dilation_rate=3,kernel_initializer='he_normal',bias_initializer='zeros',padding='same',data_format='channels_first'),
 #layers.BatchNormalization(),
    layers.MaxPooling2D(pool_size=(1,4), strides=(1, 1), padding='same',data_format='channels_first'),
 layers.Conv2D(8,kernel_size=(1,4), strides=(1,1),dilation_rate=1,kernel_initializer='he_normal',bias_initializer='zeros',padding='same',data_format='channels_first'),
 #ayers.BatchNormalization(),
    layers.MaxPooling2D(pool_size=(1,2), strides=(1, 1), padding='same',data_format='channels_first'),
 layers.Conv2D(8,kernel_size=(1,1), strides=(1,1),kernel_initializer='he_normal',bias_initializer='zeros',padding='same',data_format='channels_first'),
 #layers.BatchNormalization(),
 layers.Flatten(),
 layers.Dense(1000, activation='relu',kernel_initializer='he_normal',bias_initializer='zeros'), 
 layers.BatchNormalization(),
 layers.Dense(256, activation='relu',kernel_initializer='he_normal',bias_initializer='zeros'),     
 layers.BatchNormalization(),
 layers.Dense(128, activation='relu',kernel_initializer='he_normal',bias_initializer='zeros'),     
 layers.BatchNormalization(),
 #layers.Dense(10, activation='relu',kernel_initializer='he_normal',bias_initializer='zeros'), 
 #layers.BatchNormalization(),
 layers.Dense(1, activation=None)])

early_stopping=keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001,patience=60, verbose=0, mode='auto',baseline=None, restore_best_weights=True)
def step_decay(epoch):
    if epoch <160:
        lrate = 0.001
    else:
        lrate = 0.0005
    return lrate
lrate = keras.callbacks.LearningRateScheduler(step_decay)
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=os.path.join("fit_logs"), histogram_freq=1)
#cmd输入tensorboard --logdir fit_logs
network.compile(optimizer = 'Adam',loss=losses.mean_squared_error)
network.fit(train_db, epochs=300,validation_data=test_db,callbacks=[early_stopping,lrate,tensorboard_callback])

mse=network.evaluate(test_db)
rmse=np.sqrt(mse)
y_predict=network.predict(test_db)
y_predict=y_predict.reshape(y_predict.shape[0])
print("真实标记：\n",test_lable)
print("预测标记：\n",y_predict)
print("真实标记与预测标记的均方根误差：",rmse)

plt.plot(test_lable,'r.',y_predict,'g.')
plt.legend(["y_true", "y_predict"])



五左右：

network = Sequential([ # 封装为一个网络 layers.LeakyReLU(alpha=0.1),
 layers.Conv2D(20,kernel_size=(1,3),strides=(1,1),dilation_rate=1,kernel_initializer='he_normal',bias_initializer='zeros',padding='same',data_format='channels_first'),
 #layers.BatchNormalization(),
    layers.MaxPooling2D(pool_size=(1,2), strides=(1, 1), padding='same',data_format='channels_first'),
 layers.Conv2D(8,kernel_size=(1,5), strides=(1,1),dilation_rate=1,kernel_initializer='he_normal',bias_initializer='zeros',padding='same',data_format='channels_first'),
 #layers.BatchNormalization(),
    layers.MaxPooling2D(pool_size=(1,4), strides=(1, 1), padding='same',data_format='channels_first'),
 layers.Conv2D(8,kernel_size=(1,5),strides=(1,1),dilation_rate=1,kernel_initializer='he_normal',bias_initializer='zeros',padding='same',data_format='channels_first'),
 #layers.BatchNormalization(),
    layers.MaxPooling2D(pool_size=(1,4), strides=(1, 1), padding='same',data_format='channels_first'),
 layers.Conv2D(8,kernel_size=(1,4), strides=(1,1),dilation_rate=1,kernel_initializer='he_normal',bias_initializer='zeros',padding='same',data_format='channels_first'),
 #ayers.BatchNormalization(),
    layers.MaxPooling2D(pool_size=(1,2), strides=(1, 1), padding='same',data_format='channels_first'),
 layers.Conv2D(8,kernel_size=(1,1), strides=(1,1),kernel_initializer='he_normal',bias_initializer='zeros',padding='same',data_format='channels_first'),
 #layers.BatchNormalization(),
 layers.Flatten(),
 layers.Dense(1000, activation='relu',kernel_initializer='he_normal',bias_initializer='zeros'), 
 layers.BatchNormalization(),
 layers.Dense(256, activation='relu',kernel_initializer='he_normal',bias_initializer='zeros'),     
 layers.BatchNormalization(),
 layers.Dense(128, activation='relu',kernel_initializer='he_normal',bias_initializer='zeros'),     
 layers.BatchNormalization(),
 #layers.Dense(10, activation='relu',kernel_initializer='he_normal',bias_initializer='zeros'), 
 #layers.BatchNormalization(),
 layers.Dense(1, activation=None)])

early_stopping=keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001,patience=60, verbose=0, mode='auto',baseline=None, restore_best_weights=True)
def step_decay(epoch):
    if epoch <40:
        lrate = 0.001
    else:
        lrate = 0.0005
    return lrate
lrate = keras.callbacks.LearningRateScheduler(step_decay)
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=os.path.join("fit_logs"), histogram_freq=1)
#cmd输入tensorboard --logdir fit_logs
network.compile(optimizer = 'Adam',loss=losses.mean_squared_error)
network.fit(train_db, epochs=300,validation_data=test_db,callbacks=[early_stopping,lrate,tensorboard_callback])

mse=network.evaluate(test_db)
rmse=np.sqrt(mse)
y_predict=network.predict(test_db)
y_predict=y_predict.reshape(y_predict.shape[0])
print("真实标记：\n",test_lable)
print("预测标记：\n",y_predict)
print("真实标记与预测标记的均方根误差：",rmse)

plt.plot(test_lable,'r.',y_predict,'g.')
plt.legend(["y_true", "y_predict"])